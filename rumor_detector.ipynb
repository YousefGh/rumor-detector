{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corona_info.txt', 'r', encoding=\"utf8\") as f:\n",
    "    blob = f.read().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-4c56361686a3>:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  self.index2word_set = set(self.model.wv.index2word)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from scipy import spatial\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class TextSimilarity:\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
    "            self.index2word_set = set(self.model.wv.index2word)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "    def avg_feature_vector(self, sentence, num_features=300):\n",
    "        words = WordPunctTokenizer().tokenize(sentence)\n",
    "        feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            if word in self.index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vec = np.add(feature_vec, self.model[word])\n",
    "#         if (n_words > 0):\n",
    "#             feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec\n",
    "\n",
    "    def similarity(self, sentence1, sentence2):\n",
    "        vec1, vec2 = self.avg_feature_vector(sentence1), self.avg_feature_vector(sentence2)\n",
    "        return self.cosine_similarity(vec1, vec2)\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        return 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "sim = TextSimilarity()\n",
    "\n",
    "def find_similar(sent, blob):\n",
    "    sentences = np.asarray(sent_tokenize(blob))\n",
    "    sent_blob_sim = np.asarray([sim.similarity(sent, sentences[i]) for i in range(len(sentences))])\n",
    "    indices_sorted = np.argsort(sent_blob_sim)[::-1]\n",
    "    sent_sim_sorted = sentences[indices_sorted]\n",
    "    return sent_sim_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factual Claim: I should not contact the hospital if i felt any symptoms  \n",
      "\n",
      "Check: \n",
      "If your symptoms are severe or you feel like you need medical care, call before you go to a doctorâ€™s office, urgent care center or emergency room.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: Fever is the only COVID-19 symptom \n",
      "\n",
      "Check: \n",
      "COVID-19 symptoms include: Cough Fever Shortness of breath Muscle aches Sore throat Unexplained loss of taste or smell Diarrhea Headache   In rare cases, COVID-19 can lead to severe respiratory problems, kidney failure or death.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: diagnosing corona virus is very easy \n",
      "\n",
      "Check: \n",
      "The virus that causes COVID-19 is similar to the one that caused the 2003 SARS outbreak: both are types of coronaviruses.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: there currently exist a cure or a medication to treat corona virus \n",
      "\n",
      "Check: \n",
      "Currently, no antiviral medication is recommended to treat COVID-19 , and no cure is available for COVID-19, Antibiotics aren't effective against viral infections.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: COVID-19 doesn't lead or cause death \n",
      "\n",
      "Check: \n",
      "COVID-19 can be severe, and some cases have caused death.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: this corona virus is just like SARS \n",
      "\n",
      "Check: \n",
      "The virus that causes COVID-19 is similar to the one that caused the 2003 SARS outbreak: both are types of coronaviruses.\n",
      "----------\n",
      "\n",
      "\n",
      "Factual Claim: corona virus symptoms appears in less than 2 days \n",
      "\n",
      "Check: \n",
      "incubation period for COVID-19 It appears that symptoms are showing up in people within 14 days of exposure to the virus.\n",
      "----------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/corona_info.txt', 'r', encoding=\"utf8\") as f:\n",
    "    blob = f.read().replace(\"\\n\", \" \")\n",
    "\n",
    "with open('data/corona_questions.txt', 'r', encoding=\"utf8\") as f:\n",
    "    claims = f.read().split(\"\\n\")\n",
    "\n",
    "sw = stopwords.words('english') \n",
    "# skip_word = ['corona', 'virus', 'covid-19', 'coronavirus']\n",
    "for claim in claims:\n",
    "#     claim_filtered = ' '.join([x for x in claim.lower().split() if x not in skip_word])\n",
    "    print(\"Factual Claim:\", claim, '\\n')\n",
    "    print(\"Check:\", '\\n' + find_similar(claim, blob))\n",
    "    print(\"----------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "X =\"I love horror movies\".lower()\n",
    "Y =\"Lights out is love a horror movies\".lower()\n",
    "\n",
    "def similar_scorer(X, Y):\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X) \n",
    "    Y_list = word_tokenize(Y) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english') \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8660254037844387"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_scorer(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
